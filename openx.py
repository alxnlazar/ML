# -*- coding: utf-8 -*-
"""openX.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V480hl_jxUYeegkKfmTJnSq2qiGf-IJQ
"""

# Upload data
import gzip
import numpy as np

with gzip.open('covtype.data.gz', 'rb') as f:
    data = np.loadtxt(f, delimiter=',')

print(data.shape)

# Divide the data into features and labels
features = data[:, :-1]
labels = data[:, -1].astype(int)  # convert the label column to integer type

print("Shape of features:", features.shape)
print("Shape of labels:", labels.shape)

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data[:, :-1], data[:, -1], test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Create a Linear Regression model and fit it to the training data
model = LinearRegression()
model.fit(X_train, y_train)

# Predict the labels for the test data using the trained model
y_pred = model.predict(X_test)

# Plot the linear regression line and the actual data
plt.scatter(y_test, y_pred)
plt.plot([0, max(y_test)], [0, max(y_test)], 'k--', lw=3)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show()

# Plot the linear regression line and the actual data
plt.scatter(y_test, y_pred)
plt.plot([0, max(y_test)], [0, max(y_test)], 'k--', lw=3)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show()

# C-Support Vector Classification
from sklearn.svm import SVC
svm = SVC(kernel='linear', C=1., random_state=0)
svm.fit(X_train, y_train);
plt.figure(figsize=(8, 6))

X_all = np.vstack( (X_train, X_test) )
y_all = np.hstack( (y_train, y_test) )
plt.show()
svm.score(X_test, y_test)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Create a Logistic Regression model and fit it to the training data
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict the labels on the testing set
y_pred = model.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.ensemble import GradientBoostingRegressor

# Create a Gradient Boosting Regression model and fit it to the training data
model = GradientBoostingRegressor()
model.fit(X_train, y_train)

# Predict the target values on the testing set
y_pred = model.predict(X_test)

# Calculate the mean squared error of the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

# Normalize the data
X_train_mean = np.mean(X_train, axis=0)
X_train_std = np.std(X_train, axis=0)
X_train = (X_train - X_train_mean) / X_train_std
X_test = (X_test - X_train_mean) / X_train_std

# Define a function to create and train a neural network with given hyperparameters
def train_nn(hidden_layers, units_per_layer, activation, optimizer, epochs):
    # Define the model architecture
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Input(shape=X_train.shape[1]))
    for _ in range(hidden_layers):
        model.add(tf.keras.layers.Dense(units=units_per_layer, activation=activation))
    model.add(tf.keras.layers.Dense(units=7, activation='softmax'))

    # Compile the model
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    # Train the model
    model.fit(X_train, y_train, epochs=epochs, verbose=0)

    # Evaluate the model on the test set
    y_pred = model.predict(X_test)
    y_pred = np.argmax(y_pred, axis=1)
    acc = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)

    return acc, cm

    # Define a list of hyperparameters to search over
hidden_layers_list = [1, 2, 3]
units_per_layer_list = [16, 32, 64]
activation_list = ['relu', 'sigmoid']
optimizer_list = [tf.keras.optimizers.Adam, tf.keras.optimizers.RMSprop]
epochs_list = [10, 20, 50]

# Perform a grid search over the hyperparameters
best_acc = 0
for hidden_layers in hidden_layers_list:
    for units_per_layer in units_per_layer_list:
        for activation in activation_list:
            for optimizer in optimizer_list:
                for epochs in epochs_list:
                    acc, cm = train_nn(hidden_layers, units_per_layer, activation, optimizer(), epochs)
                    if acc > best_acc:
                        best_acc = acc
                        best_cm = cm
                        best_hyperparams = {'hidden_layers': hidden_layers,
                                            'units_per_layer': units_per_layer,
                                            'activation': activation,
                                            'optimizer': optimizer.__name__,
                                            'epochs': epochs}

# Print the best hyperparameters and the corresponding test set performance
print("Best hyperparameters:", best_hyperparams)
print("Test set accuracy:", best_acc)
print("Confusion matrix:\n", best_cm)

# Plot training curves
plt.plot(history.history['accuracy'], label='train accuracy')
plt.plot(history.history['val_accuracy'], label='val accuracy')
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Training Curves')
plt.xlabel('Epochs')
plt.legend()
plt.show()

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)

print('Test Loss:', test_loss)
print('Test Accuracy:', test_accuracy)

#This will print the test loss and test accuracy of the trained neural network on the test set. We can compare these metrics with the validation metrics obtained during hyperparameter tuning to get an idea of how well the model is generalizing.

from flask import Flask, request, jsonify
import joblib

app = Flask(__name__)

# Load models
reg_model = joblib.load('reg_model.joblib')
lin_model = joblib.load('lin_model.joblib')
svm_model = joblib.load('svm_model.joblib')
gb_model = joblib.load('gb_model.joblib')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    
    # Extract features from request
    feature1 = data['feature1']
    feature2 = data['feature2']
    feature3 = data['feature3']
    # ... and so on for all features
    
    # Create feature array
    features = [[feature1, feature2, feature3, ...]]
    
    # Choose model based on request
    model = None
    if data['model'] == 'reg':
        model = reg_model
    elif data['model'] == 'lin':
        model = lin_model
    elif data['model'] == 'svm':
        model = svm_model
    elif data['model'] == 'gb':
        model = gb_model
    else:
        return jsonify({'error': 'Invalid model choice'})
    
    # Make prediction and return result as JSON
    prediction = model.predict(features)
    return jsonify({'prediction': prediction.tolist()})

if __name__ == '__main__':
    app.run(debug=True)